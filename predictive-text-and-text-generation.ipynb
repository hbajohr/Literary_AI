{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Text generation with Markov chains\n",
    "\n",
    "Adapted from a notebook by [Allison Parrish](http://www.decontextualize.com/)\n",
    "\n",
    "In our last workshop, we looked at programs that use simple if-then rules to string together sentences. This is not the only way to make text. Another one calculates the statistical distribution of characters in a text. This is also the basis for modern machine learning, but before we look at this, we will learn about a much older process: Markov chains.\n",
    "\n",
    "Markov chains are a simple example of *predictive text generation*. It is a method of text generation that make use of a statistical model that, given a certain stretch of text, *predicts* which bit of text should come next, based on probabilities learned from an existing corpus of text. It was first discussed by Russian mathematician Andrey Markov in 1906, but became an important basis for information theory, particularly that of Claude Shannon (more about him next week). For the moment, it is enough to understand that statistic modeling of text *only looks at the statistical distribution of characters or words, without any regard to their meaning*. However, as the Markov chain shows, the text that is produces without such meaning quickly becomes natural-sounding nevertheless. \n",
    "\n",
    "Of course, Markov chains are incredibly primitive compared to neural networks. But they allow one to develop an intuition for one of the most powerful ideas of modern natural language generation: That language can be synthesized without any regard to meaning, simply by using statistics. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with text files\n",
    "\n",
    "Before we get started, we'll first need some text! Grab two [plain text files from Project Gutenberg](http://www.gutenberg.org/) (or from another source of your choice) and save them to the same directory as this notebook. (If you get to this notebook through the link on bCourses, the files should already be in your root folder.) The code in the following cell loads into Python variables the contents of *two plain text files*, assigned to variables `text_a` and `text_b`. You'll need to replace the filenames with the names of the files that you downloaded, keeping the quotation marks (`\"`) intact. Or you can just use the ones that I provided and change nothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "text_a = open(\"1342-0.txt\").read()\n",
    "text_b = open(\"84-0.txt\").read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These variables are *strings*, which are essentially just long lists of the characters that occur in the text, in the order that they occur. The code in the following cell shows the first two hundred characters of text A:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "﻿The Project Gutenberg EBook of Pride and Prejudice, by Jane Austen\n",
      "\n",
      "This eBook is for the use of anyone anywhere at no cost and with\n",
      "almost no restrictions whatsoever.  You may copy it, give it away \n"
     ]
    }
   ],
   "source": [
    "print(text_a[:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can change `text_a` to `text_b` to see the output from your second text, or change `200` to a number of your choosing.\n",
    "\n",
    "The `random.sample()` function gives us a random sampling of the contents of a variable (as long as that variable is a sequence of things, like a string or a list). So, for example, to see twenty random characters from text B:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['t',\n",
       " 'd',\n",
       " 'i',\n",
       " 'h',\n",
       " ' ',\n",
       " 's',\n",
       " 's',\n",
       " 'r',\n",
       " 'e',\n",
       " 'r',\n",
       " 'i',\n",
       " 'r',\n",
       " 'd',\n",
       " 's',\n",
       " 'g',\n",
       " 'v',\n",
       " 'u',\n",
       " 'd',\n",
       " 'i',\n",
       " 't']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "random.sample(text_b, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This isn't incredibly helpful on its own, but you'll notice that the characters it drew (probably) more or less follow the expected letter distribution for English (i.e., lots of `e`s and `n`s and `t`s)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perhaps more interesting would be to see a randomly-sampled list of *words*. To do this, we'll make separate variables for the words in the text, using a Python function called `.split()`, which takes a string and turns it into a list of words contained in that string. The following cell makes two new variables that contain the words from both texts respectively:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "a_words = text_a.split()\n",
    "b_words = text_b.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, ten random words from both text A and text B:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['listened',\n",
       " 'a',\n",
       " 'see',\n",
       " 'absence',\n",
       " 'listening',\n",
       " 'she',\n",
       " 'as',\n",
       " 'exhibit.”',\n",
       " 'Fitzwilliam,',\n",
       " 'state']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.sample(a_words, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['saw',\n",
       " 'shepherd.',\n",
       " 'only',\n",
       " 'rustic,',\n",
       " 'being',\n",
       " 'resolved',\n",
       " 'near',\n",
       " 'would',\n",
       " 'feel',\n",
       " 'with']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.sample(b_words, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code in the following cell uses Python's `Counter` object to count the *most common* letters in the first of these texts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(' ', 113941),\n",
       " ('e', 70344),\n",
       " ('t', 47283),\n",
       " ('a', 42156),\n",
       " ('o', 41138),\n",
       " ('n', 38430),\n",
       " ('i', 36273),\n",
       " ('h', 33883),\n",
       " ('r', 33293),\n",
       " ('s', 33292),\n",
       " ('d', 22247),\n",
       " ('l', 21282)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "Counter(text_a).most_common(12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specifying the `a_words` variable gives the most frequent *words* instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 4205),\n",
       " ('to', 4121),\n",
       " ('of', 3662),\n",
       " ('and', 3309),\n",
       " ('a', 1945),\n",
       " ('her', 1858),\n",
       " ('in', 1813),\n",
       " ('was', 1795),\n",
       " ('I', 1740),\n",
       " ('that', 1419),\n",
       " ('not', 1356),\n",
       " ('she', 1306)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(a_words).most_common(12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare these to the most common words in text B:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 4056),\n",
       " ('and', 2971),\n",
       " ('of', 2741),\n",
       " ('I', 2719),\n",
       " ('to', 2142),\n",
       " ('my', 1631),\n",
       " ('a', 1394),\n",
       " ('in', 1125),\n",
       " ('was', 993),\n",
       " ('that', 987),\n",
       " ('with', 700),\n",
       " ('had', 679)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(b_words).most_common(12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markov models\n",
    "\n",
    "The question a Markov chain tries to answer is this: Given a stretch of text (say a string of characters, or run of words), what is most the most likely bit of text to come *next*?\n",
    "\n",
    "One way to answer this question is with an *n-gram* based Markov model. What's an n-gram? I'm glad you asked!\n",
    "\n",
    "### N-grams\n",
    "\n",
    "An n-gram is simply a sequence of units drawn from a longer sequence; in the case of text, the unit in question is usually a *character* or a *word*. For convenience, we'll call the unit of the n-gram is called its level; the length of the n-gram is called its order. For example, the following is a list of all unique character-level order-2 n-grams in the word ´condescendences´:\n",
    "\n",
    "    co\n",
    "    on\n",
    "    nd\n",
    "    de\n",
    "    es\n",
    "    sc\n",
    "    ce\n",
    "    en\n",
    "    nc\n",
    "\n",
    "And the following is an excerpt from the list of all unique word-level order-5 n-grams in The Road Not Taken:\n",
    "\n",
    "    Two roads diverged in a\n",
    "    roads diverged in a yellow\n",
    "    diverged in a yellow wood,\n",
    "    in a yellow wood, And\n",
    "    a yellow wood, And sorry\n",
    "    yellow wood, And sorry I\n",
    "\n",
    "N-grams are used frequently in natural language processing and are a basic tool text analysis. Their applications range from programs that correct spelling (since ´the´ is more likely than ´teh´, you can autocorrect according to probablities of character distribution) to creative visualizations to compression algorithms to stylometrics to, like here, generative text.\n",
    "\n",
    "### What comes next?\n",
    "\n",
    "A Markov model for text begins with a list of n-grams. But in addition to making this list, we also keep track of what unit of text (word, character, etc.) *follows* each of those n-grams.\n",
    "\n",
    "Let’s do a quick example by hand. This is the same character-level order-2 n-gram analysis of the (very brief) text “condescendences” as above, but this time keeping track of all characters that follow each n-gram:\n",
    "\n",
    "| n-grams |\tnext? |\n",
    "| ------- | ----- |\n",
    "|co| n|\n",
    "|on| d|\n",
    "|nd| e, e|\n",
    "|de| s, n|\n",
    "|es| c, (end of text)|\n",
    "|sc| e|\n",
    "|ce| n, s|\n",
    "|en| d, c|\n",
    "|nc| e|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this table, we can determine that while the n-gram `co` is followed by n 100% of the time, and while the n-gram `on` is followed by `d` 100% of the time, the n-gram `de` is followed by `s` 50% of the time, and `n` the rest of the time. Likewise, the n-gram `es` is followed by `c` 50% of the time, and followed by the end of the text the other 50% of the time.\n",
    "\n",
    "Exercise: Imagine (or even better, write out) what this table might look like if you were analyzing words instead of characters, with a source text of your choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Markov chains: Generating text from a Markov model\n",
    "\n",
    "The Markov models we created above don't just give us interesting statistical probabilities. It also allows us generate a *new* text with those probabilities by *chaining together predictions*. Here’s how we’ll do it, starting with the order 2 character-level Markov model of `condescendences`: (1) start with the initial n-gram (`co`)—those are the first two characters of our output. (2) Now, look at the last *n* characters of output, where *n* is the order of the n-grams in our table, and find those characters in the “n-grams” column. (3) Choose randomly among the possibilities in the corresponding “next” column, and append that letter to the output. (Sometimes, as with `co`, there’s only one possibility). (4) If you chose “end of text,” then the algorithm is over. Otherwise, repeat the process starting with (2). Here’s a record of the algorithm in action:\n",
    "\n",
    "    co\n",
    "    con\n",
    "    cond\n",
    "    conde\n",
    "    conden\n",
    "    condend\n",
    "    condendes\n",
    "    condendesc\n",
    "    condendesce\n",
    "    condendesces\n",
    "    \n",
    "As you can see, we’ve come up with a word that *looks* like the original word, and could even be passed off as a genuine English word (if you squint at it). *From a statistical standpoint, the output of our algorithm is nearly indistinguishable from the input.* This kind of algorithm—moving from one state to the next, according to a list of probabilities—is known as a Markov chain generator.\n",
    "\n",
    "### Generating with Markovify\n",
    "\n",
    "Fortunately, with the invention of digital computers, you don't have to perform this algorithm by hand! In fact, Markov chain text generation has been a pastime of poets and programmers going back [all the way to 1983](https://www.jstor.org/stable/24969024), so it should be no surprise that there are many implementations of the idea in Python that you can download and install. The one we're going to use is [Markovify](https://github.com/jsvine/markovify), a Markov chain text generation library originally developed for BuzzFeed, apparently. It comes with a lot of extra niceties that will make our lives easier, but underneath the hood, it implements an algorithm very similar to the one we just did by hand above.\n",
    "\n",
    "To install Markovify on your computer, run the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting markovify\n",
      "  Using cached markovify-0.9.4-py3-none-any.whl\n",
      "Collecting unidecode\n",
      "  Using cached Unidecode-1.3.8-py3-none-any.whl (235 kB)\n",
      "Installing collected packages: unidecode, markovify\n",
      "Successfully installed markovify-0.9.4 unidecode-1.3.8\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install markovify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then run this cell to make the library available in your notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import markovify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code in the following cell creates a new text generator, using the text in the variable specified to build the Markov model, which is then assigned to the variable `generator_a`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "generator_a = markovify.Text(text_a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can then call the `.make_sentence()` method to generate a sentence from the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It was evident that she could reply to the contrary himself.\n"
     ]
    }
   ],
   "source": [
    "print(generator_a.make_sentence())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `.make_short_sentence()` method allows you to specify a maximum length for the generated sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Were the whole affair.\n"
     ]
    }
   ],
   "source": [
    "print(generator_a.make_short_sentence(50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, Markovify tries to generate a sentence that is significantly different from any existing sentence in the input text. As a consequence, sometimes the `.make_sentence()` or `.make_short_sentence()` methods will return `None`, which means that in ten tries it wasn't able to generate such a sentence. You can work around this by increasing the number of times it tries to generate a sufficiently unique sentence using the `tries` parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mr. Gardiner, though unknown.\n"
     ]
    }
   ],
   "source": [
    "print(generator_a.make_short_sentence(40, tries=100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or by disabling the check altogether with `test_output=False` (note that this means the generator will occasionally return stretches of text that are present in the source text):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is nothing to guide us.\n"
     ]
    }
   ],
   "source": [
    "print(generator_a.make_short_sentence(40, test_output=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changing the order\n",
    "\n",
    "When you create the model, you can specify the order of the model using the `state_size` parameter. It defaults to 2. Let's make two model with different orders and compare:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gen_a_1 = markovify.Text(text_a, state_size=1)\n",
    "gen_a_4 = markovify.Text(text_a, state_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "order 1\n",
      "I have admittance.\n",
      "\n",
      "order 4\n",
      "In spite of what her sister must endure.\n"
     ]
    }
   ],
   "source": [
    "print(\"order 1\")\n",
    "print(gen_a_1.make_sentence(test_output=False))\n",
    "print()\n",
    "print(\"order 4\")\n",
    "print(gen_a_4.make_sentence(test_output=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, the higher the order, the more the sentences will seem \"coherent\" (i.e., more closely resembling the source text). Lower order models will produce more variation. Deciding on the order is usually a matter of taste and trial-and-error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changing the level\n",
    "\n",
    "Markovify, by default, works with *words* as the individual unit. It doesn't come out-of-the-box with support for character-level models. The following code defines a new kind of Markovify generator that implements character-level models. Execute it before continuing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class SentencesByChar(markovify.Text):\n",
    "    def word_split(self, sentence):\n",
    "        return list(sentence)\n",
    "    def word_join(self, words):\n",
    "        return \"\".join(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any of the parameters you passed to `markovify.Text` you can also pass to `SentencesByChar`. The `state_size` parameter still controls the order of the model, but now the n-grams are characters, not words.\n",
    "\n",
    "The following cell implements a character-level Markov text generator for the word \"condescendences\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "con_model = SentencesByChar(\"condescendences\", state_size=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute the cell below to see the output—it'll be a lot like what we implemented by hand earlier!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'condescencences'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "con_model.make_sentence()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, you can use a character-level model on any text of your choice. So, for example, the following cell creates a character-level order-7 Markov chain text generator from text A:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gen_a_char = SentencesByChar(text_a, state_size=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the cell below prints out a random sentence from this generator. (The `.replace()` is to get rid of any newline characters in the output.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It was very much, I feel it my duty to the ground, determined.\n"
     ]
    }
   ],
   "source": [
    "print(gen_a_char.make_sentence(test_output=False).replace(\"\\n\", \" \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining models\n",
    "\n",
    "Markovify has a handy feature that allows you to *combine* models, creating a new model that draws on probabilities from both of the source models. You can use this to create hybrid output that mixes the style and content of two (or more!) different source texts. To do this, you need to create the models independently, and then call `.combine()` to combine them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "generator_a = markovify.Text(text_a)\n",
    "generator_b = markovify.Text(text_b)\n",
    "combo = markovify.combine([generator_a, generator_b], [0.5, 0.5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bit of code `[0.5, 0.5]` controls the \"weights\" of the models, i.e., how much to emphasize the probabilities of any model. You can change this to suit your tastes. (E.g., if you want mostly text A with but a *soupçon* of text B, you would write `[0.9, 0.1]`. Try it!) \n",
    "\n",
    "Then you can create sentences using the combined model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "But I concealed my struggles, and flattered you into the garden and the principal spokesman, and Miss Lucas, whose civility in listening to him how absolutely necessary to interrupt my tranquillity.\n"
     ]
    }
   ],
   "source": [
    "print(combo.make_sentence())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bringing it all together\n",
    "\n",
    "I've pre-written some code below to make it easy for you to experiment and produce output from Markovify. Just make adjustments to the values assigned to the variables in the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# change to \"word\" for a word-level model\n",
    "level = \"char\"\n",
    "# controls the length of the n-gram\n",
    "order = 7\n",
    "# controls the number of lines to output\n",
    "output_n = 14\n",
    "# weights between the models; text A first, text B second.\n",
    "# if you want to completely exclude one model, set its corresponding value to 0\n",
    "weights = [0.5, 0.5]\n",
    "# limit sentence output to this number of characters\n",
    "length_limit = 280"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(The lines beginning with `#` are \"comments\"—they don't do anything, they're just there to explain what's happening in the code.)\n",
    "\n",
    "After making your changes above, run the cell below to generate text according to your parameters. Repeat as necessary until you get something you really like!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mr. Darcy was commenced, however, Felix flitted before Monday: as soon communications.\n",
      "\n",
      "I found herself.\n",
      "\n",
      "As he was no horsewoman, about it is done, and on which are to her former occupations, for it had been insufficiently that urgent business.\n",
      "\n",
      "We were some new observe her temper was not on hearing amidst his early a hundred pounds, and was consolation than Jane.\n",
      "\n",
      "They found that the notice of time, the whole works provide for the work of my condition to this attentions.\n",
      "\n",
      "I was guiltless, but my though perhaps, I had known as you had any value he put his repeatedly tried to Mr. Bingley had been industriously, what degree of his cousin, why is not the least could do.\n",
      "\n",
      "The first an angel become more interesting in this agreed to discover whisper than they always been massacred before been a week since my journey.\n",
      "\n",
      "She received the purport of your countenance.\n",
      "\n",
      "We possessed much good angel, 28th March, 17—.   So stranger, and Elizabeth passion and again I shall curse upon the being the wretch whom all like it, has been at Netherfield, and I will doubtless terrific trial began to find other alteration, and its owner.\n",
      "\n",
      "Mrs. Gardiner.\n",
      "\n",
      "Covered the earth a dæmon whom the harbour, but there was discontinued talk again; coaxed and talk on with an alacrity which she had quitted their new friends that can be notice.\n",
      "\n",
      "I am sure I do not neglected to her and living and obsequious civility, and I must desire of reflected into her music.\n",
      "\n",
      "She had the point and plenty reigned not to have been unreasonable relief, from the sullen despair upon the best and confidence, she acceded with him, I believed it to one of his own nature.\n",
      "\n",
      "But still my guiding spirit to take leaves had dedicate myself enterprise.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_cls = markovify.Text if level == \"word\" else SentencesByChar\n",
    "gen_a = model_cls(text_a, state_size=order)\n",
    "gen_b = model_cls(text_b, state_size=order)\n",
    "gen_combo = markovify.combine([gen_a, gen_b], weights)\n",
    "for i in range(output_n):\n",
    "    out = gen_combo.make_short_sentence(length_limit, test_output=False)\n",
    "    out = out.replace(\"\\n\", \" \")\n",
    "    print(out)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating with non-prose text\n",
    "\n",
    "Markovify assumes you're feeding it prose, i.e., a text file that can be parsed into sentences by separating on sentence-ending punctuation. But often you're *not* working with text like this. For example, let's generate some sonnets. First, download [this plaintext version of Shakespeare's sonnets](https://raw.githubusercontent.com/aparrish/plaintext-example-files/master/sonnets.txt) and keep it in the same directory as this notebook. We'll define the sonnet-generating task as consisting of (a) training a Markov chain on lines of poetry and then (b) generating a sequence of fourteen lines of poetry. Since the *line* is the unit now and not the *sentence*, we need to use Markovify's `NewlineText` class instead of `Text`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "sonnets_text = open(\"sonnets.txt\").read()\n",
    "sonnets_model = markovify.NewlineText(sonnets_text, state_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then we can generate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Flatter the summer and do not sick Muse in thy steel bosom's ward,\""
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sonnets_model.make_sentence()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now make a sonnet, sorta:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As any wrinkle graven there;\n",
      "While comments of spirit affords,\n",
      "O! how shall not to stay and idle rank before.\n",
      "And so much hold,\n",
      "To find out even by mutual ordering;\n",
      "And do not with his golden time.\n",
      "Then need blood;\n",
      "Lo! thus, by this line some mother.\n",
      "Hers by the wise world may stain both moon and truth;\n",
      "And swear against my slumbers should blunter be gone,\n",
      "So do if thou see'st the thanks, if thou art,\n",
      "To leave poor infant's discontent;\n",
      "Full character'd with loss,\n",
      "For then find,\n"
     ]
    }
   ],
   "source": [
    "for i in range(14):\n",
    "    print(sonnets_model.make_sentence())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doing this with a character-level model is a bit more tricky. I've written code in the cell below that defines a new class, `LinesByCharacter` that works like `NewlineText` but operates character-by-character instead of word-by-word:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinesByChar(markovify.NewlineText):\n",
    "    def word_split(self, sentence):\n",
    "        return list(sentence)\n",
    "    def word_join(self, words):\n",
    "        return \"\".join(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create a character model with the sonnets, line by line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "sonnets_char_model = LinesByChar(sonnets_text, state_size=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And generate new sonnets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When I soul provided make soul a for they had not Time's rose, hath to my mistrengthen inwards expresence, being friend\n",
      "From thy fair whom is away,\n",
      "With which to thee fair fame that, nor drowning?\n",
      "For stol'n of small making wood repose wherevery bles brief\n",
      "That would my ruth,\n",
      "And kingle life rude lack again,\n",
      "And in me the earse,\n",
      "Then make world, or rime:\n",
      "In the hunter inflame hidden usure,\n",
      "Yours are there a sad die as and thy outlive?\n",
      "O! let me world's eyes,\n",
      "When soul's trange is admire\n",
      "Call'd fruit;\n",
      "And the to their broke, as many, see, what dote,\n"
     ]
    }
   ],
   "source": [
    "for i in range(14):\n",
    "    print(sonnets_char_model.make_sentence())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New moods\n",
    "\n",
    "Character-level Markov chains are especially suitable, in my experience, for generating shorter texts, like individual words or names. Let's generate names of new moods using this technique. First, download [this JSON file of moods](https://raw.githubusercontent.com/dariusk/corpora/9bb62927951f79bec2454f29d71b6e9b28d874b1/data/humans/moods.json) from [Corpora Project](https://github.com/dariusk/corpora/) and save to the same directory as this notebook. (A JSON file is a specific way of structuring data. Think of it as an excel file: it can have lists with several colums and rows.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then load the JSON file and grab just the list of words naming moods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "mood_data = json.loads(open(\"./moods.json\").read())\n",
    "moods = mood_data['content']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The easiest way to use this is to make one big string with the moods joined together with newlines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "moods_text = \"\\n\".join(moods)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then use `LinesByChar` to make the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "moods_char_model = LinesByChar(moods_text, state_size=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And voila, just based on the character disctribution of the list of existing moods, Markovify creates new, plausible moods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mourned\n",
      "exploiteful\n",
      "trainepted\n",
      "trative\n",
      "critive\n",
      "kiddy\n",
      "melanced\n",
      "attacted\n",
      "exhauseated\n",
      "relitted\n",
      "yieldisbelittened\n",
      "disty\n",
      "jubilankful\n",
      "defendeciated\n",
      "mellous\n",
      "emble\n",
      "hyperky\n",
      "wholistificult\n",
      "grievolemned\n",
      "carefressed\n",
      "infered\n",
      "posabothed\n",
      "senterplearful\n",
      "expeciated\n"
     ]
    }
   ],
   "source": [
    "for i in range(24):\n",
    "    print(moods_char_model.make_sentence())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can try this with other lists – for instance, try using the file \"firstnames.json\" instead of \"moods.json\" to create new and exciting first names!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
